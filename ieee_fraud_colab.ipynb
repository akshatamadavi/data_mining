{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2055d36",
      "metadata": {
        "id": "d2055d36"
      },
      "source": [
        "# Colab-ready: AutoGluon + IEEE-CIS Fraud Detection\n",
        "\n",
        "This notebook combines the steps from the project's `tabular-kaggle.ipynb` and the existing Colab helper into a single, runnable Colab notebook. It will:\n",
        "- install packages,\n",
        "- show two ways to provide your `kaggle.json` (upload or Drive),\n",
        "- download the dataset via the Kaggle API,\n",
        "- merge CSVs, reduce memory, and optionally sample for quick runs,\n",
        "- train AutoGluon,\n",
        "- save predictions and optionally submit to Kaggle via the CLI.\n",
        "\n",
        "Run cells in order. New cells don't need IDs but existing ones do — this file is freshly created so cells have no `metadata.id`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1718c065",
      "metadata": {
        "id": "1718c065"
      },
      "source": [
        "## 1 — Install dependencies\n",
        "Run this cell first in Colab. It installs `kaggle` and `autogluon.tabular` plus common data packages. If any install fails, try a different AutoGluon version that matches Colab's Python runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c0e08467",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0e08467",
        "outputId": "c682b3b4-3265-47f4-e5fa-38a56022a942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run in Colab)\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install kaggle autogluon.tabular pandas numpy scikit-learn\n",
        "import sys, pandas as pd, numpy as np\n",
        "print('Python:', sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9019ee",
      "metadata": {
        "id": "0c9019ee"
      },
      "source": [
        "## 2 — Provide your Kaggle API key (`kaggle.json`)\n",
        "\n",
        "Two recommended options:\n",
        "A) Upload interactively (one-off): use the upload cell below.\n",
        "B) Mount Google Drive (recommended for repeat runs): put `kaggle.json` in `MyDrive/.kaggle/kaggle.json` then run the Drive cell to copy it to `~/.kaggle/kaggle.json`.\n",
        "\n",
        "Important: never commit `kaggle.json` to source control. It contains your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83570af6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "83570af6",
        "outputId": "aef68865-886e-419b-bb1b-a8332bfd8ffd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-473cf8a2-c919-4d07-a759-f0d2e413f6de\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-473cf8a2-c919-4d07-a759-f0d2e413f6de\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Saved to ~/.kaggle/kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Option A: Upload kaggle.json (interactive)\n",
        "from google.colab import files\n",
        "import os\n",
        "uploaded = files.upload()\n",
        "if 'kaggle.json' in uploaded:\n",
        "    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
        "    open(os.path.expanduser('~/.kaggle/kaggle.json'), 'wb').write(uploaded['kaggle.json'])\n",
        "    os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
        "    print('Saved to ~/.kaggle/kaggle.json')\n",
        "else:\n",
        "    print('No kaggle.json uploaded. Use Drive option if preferred.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eaa8be6",
      "metadata": {
        "id": "0eaa8be6"
      },
      "outputs": [],
      "source": [
        "# Option B: Mount Google Drive and copy kaggle.json from MyDrive/.kaggle/\n",
        "from google.colab import drive\n",
        "import shutil, os\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/.kaggle/kaggle.json'\n",
        "if os.path.exists(drive_path):\n",
        "    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
        "    shutil.copy(drive_path, os.path.expanduser('~/.kaggle/kaggle.json'))\n",
        "    os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
        "    print('Copied kaggle.json from Drive to ~/.kaggle/kaggle.json')\n",
        "else:\n",
        "    print('No kaggle.json found in Drive at', drive_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a9691d6",
      "metadata": {
        "id": "5a9691d6"
      },
      "source": [
        "## 3 — Download dataset via Kaggle API\n",
        "This will download and unzip the competition data into `/content/data`. Ensure you have accepted the competition rules on the Kaggle website first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "73feb9b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73feb9b0",
        "outputId": "80e129d9-d7c5-40e1-d6d3-d40e3278a622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Files:\n",
            "total 1.5G\n",
            "-rw-r--r-- 1 root root  30M Mar 19  2021 california-house-prices.zip\n",
            "-rw-r--r-- 1 root root 119M Dec 11  2019 ieee-fraud-detection.zip\n",
            "-rw-r--r-- 1 root root 248K Mar 19  2021 sample_submission.csv\n",
            "-rw-r--r-- 1 root root  35M Mar 19  2021 test.csv\n",
            "-rw-r--r-- 1 root root  25M Dec 11  2019 test_identity.csv\n",
            "-rw-r--r-- 1 root root 585M Dec 11  2019 test_transaction.csv\n",
            "-rw-r--r-- 1 root root  51M Mar 19  2021 train.csv\n",
            "-rw-r--r-- 1 root root  26M Dec 11  2019 train_identity.csv\n",
            "-rw-r--r-- 1 root root 652M Dec 11  2019 train_transaction.csv\n"
          ]
        }
      ],
      "source": [
        "# Download and unzip\n",
        "competition = 'california-house-prices'\n",
        "import os\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "!kaggle competitions download -c $competition -p /content/data --quiet\n",
        "!unzip -oq /content/data/{competition}.zip -d /content/data || true\n",
        "print('Done. Files:')\n",
        "!ls -lh /content/data | sed -n '1,120p'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad77cba6",
      "metadata": {
        "id": "ad77cba6"
      },
      "source": [
        "## 4 — Load, merge, and reduce memory\n",
        "We left-join transaction with identity on `TransactionID`. We also include a utility to downcast numeric columns to reduce memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b72d11a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b72d11a8",
        "outputId": "987dc18e-0616-4ddb-9fc0-b3e669f6943f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_transaction -> True\n",
            "train_identity -> True\n",
            "test_transaction -> True\n",
            "test_identity -> True\n",
            "sample_submission -> True\n",
            "Memory: 2,513.97 MB -> 1,603.31 MB\n",
            "Memory: 2,164.10 MB -> 1,386.12 MB\n",
            "train shape (590540, 434) test shape (506691, 433)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, numpy as np, os\n",
        "DATA_DIR = '/content/data'\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if pd.api.types.is_numeric_dtype(col_type):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if pd.api.types.is_integer_dtype(col_type):\n",
        "                if c_min >= 0:\n",
        "                    if c_max < 255:\n",
        "                        df[col] = df[col].astype(np.uint8)\n",
        "                    elif c_max < 65535:\n",
        "                        df[col] = df[col].astype(np.uint16)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.uint32)\n",
        "                else:\n",
        "                    df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "            else:\n",
        "                df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    if verbose:\n",
        "        print(f'Memory: {start_mem:,.2f} MB -> {end_mem:,.2f} MB')\n",
        "    return df\n",
        "# Read files\n",
        "paths = {\n",
        "    'train_transaction': os.path.join(DATA_DIR, 'train_transaction.csv'),\n",
        "    'train_identity': os.path.join(DATA_DIR, 'train_identity.csv'),\n",
        "    'test_transaction': os.path.join(DATA_DIR, 'test_transaction.csv'),\n",
        "    'test_identity': os.path.join(DATA_DIR, 'test_identity.csv'),\n",
        "    'sample_submission': os.path.join(DATA_DIR, 'sample_submission.csv'),\n",
        "}\n",
        "for k,p in paths.items():\n",
        "    print(k, '->', os.path.exists(p))\n",
        "train_tr = pd.read_csv(paths['train_transaction'], low_memory=False)\n",
        "train_id = pd.read_csv(paths['train_identity'], low_memory=False)\n",
        "test_tr  = pd.read_csv(paths['test_transaction'], low_memory=False)\n",
        "test_id  = pd.read_csv(paths['test_identity'], low_memory=False)\n",
        "train = train_tr.merge(train_id, on='TransactionID', how='left')\n",
        "test  = test_tr.merge(test_id, on='TransactionID', how='left')\n",
        "train = reduce_mem_usage(train)\n",
        "test = reduce_mem_usage(test)\n",
        "print('train shape', train.shape, 'test shape', test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cdfb158",
      "metadata": {
        "id": "6cdfb158"
      },
      "source": [
        "## 5 — Optional: sample for quicker experiments\n",
        "Set `USE_SAMPLE=True` to run a quick baseline. This helps when Colab runs out of RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e79fb09",
      "metadata": {
        "id": "0e79fb09"
      },
      "outputs": [],
      "source": [
        "USE_SAMPLE = False\n",
        "SAMPLE_FRAC = 0.1\n",
        "if USE_SAMPLE:\n",
        "    train = train.sample(frac=SAMPLE_FRAC, random_state=42)\n",
        "    print('Sampled train shape:', train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40b0737",
      "metadata": {
        "id": "b40b0737"
      },
      "source": [
        "## 6 — Train AutoGluon TabularPredictor\n",
        "We use `isFraud` as the label. Adjust `time_limit` and `presets` as needed. For large jobs, save the model directory to Drive after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0a22ac64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a22ac64",
        "outputId": "823cab2b-d185-4f57-acfa-ca2c3538baf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          8\n",
            "Memory Avail:       42.20 GB / 50.99 GB (82.8%)\n",
            "Disk Space Avail:   184.93 GB / 225.83 GB (81.9%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Using hyperparameters preset: hyperparameters='default'\n",
            "Beginning AutoGluon training ... Time limit = 1800s\n",
            "AutoGluon will save models to \"/content/ag_models\"\n",
            "Train Data Rows:    590540\n",
            "Train Data Columns: 432\n",
            "Label Column:       isFraud\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [np.uint8(0), np.uint8(1)]\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    43775.19 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1622.64 MB (3.7% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 4): ['V28', 'V154', 'V155', 'V156']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 4 | ['V28', 'V154', 'V155', 'V156']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])    :   2 | ['TransactionDT', 'card1']\n",
            "\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t('float', [])    : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])      :   2 | ['TransactionDT', 'card1']\n",
            "\t13.5s = Fit runtime\n",
            "\t428 features in original data used to generate 428 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 911.81 MB (2.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 15.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 584634, Val Rows: 5906\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 1784.84s of the 1784.84s of remaining time.\n",
            "\tFitting with cpus=4, gpus=0, mem=5.0/40.2 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's binary_logloss: 0.070524\n",
            "[2000]\tvalid_set's binary_logloss: 0.0621083\n",
            "[3000]\tvalid_set's binary_logloss: 0.0567775\n",
            "[4000]\tvalid_set's binary_logloss: 0.0541112\n",
            "[5000]\tvalid_set's binary_logloss: 0.0516139\n",
            "[6000]\tvalid_set's binary_logloss: 0.0503407\n",
            "[7000]\tvalid_set's binary_logloss: 0.0496697\n",
            "[8000]\tvalid_set's binary_logloss: 0.0496936\n",
            "[9000]\tvalid_set's binary_logloss: 0.0495574\n",
            "[10000]\tvalid_set's binary_logloss: 0.0498254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t0.9691\t = Validation score   (roc_auc)\n",
            "\t477.4s\t = Training   runtime\n",
            "\t1.83s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 1305.21s of the 1305.21s of remaining time.\n",
            "\tFitting with cpus=4, gpus=0, mem=5.0/39.0 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's binary_logloss: 0.0636254\n",
            "[2000]\tvalid_set's binary_logloss: 0.0554408\n",
            "[3000]\tvalid_set's binary_logloss: 0.0506679\n",
            "[4000]\tvalid_set's binary_logloss: 0.048336\n",
            "[5000]\tvalid_set's binary_logloss: 0.0471347\n",
            "[6000]\tvalid_set's binary_logloss: 0.0468823\n",
            "[7000]\tvalid_set's binary_logloss: 0.0473612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t0.9707\t = Validation score   (roc_auc)\n",
            "\t361.41s\t = Training   runtime\n",
            "\t0.76s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ... Training model for up to 942.81s of the 942.81s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.4/38.9 GB\n",
            "\t0.9344\t = Validation score   (roc_auc)\n",
            "\t237.0s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 705.17s of the 705.17s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.4/38.8 GB\n",
            "\t0.9365\t = Validation score   (roc_auc)\n",
            "\t185.79s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 518.76s of the 518.75s of remaining time.\n",
            "\tFitting with cpus=4, gpus=0, mem=5.6/38.7 GB\n",
            "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 516.77s of the 516.77s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.4/38.7 GB\n",
            "\t0.9041\t = Validation score   (roc_auc)\n",
            "\t160.71s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 355.43s of the 355.43s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.4/38.7 GB\n",
            "\t0.9216\t = Validation score   (roc_auc)\n",
            "\t162.36s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 190.00s of the 190.00s of remaining time.\n",
            "\tFitting with cpus=4, gpus=0, mem=8.9/38.6 GB\n",
            "\tRan out of time, stopping training early. (Stopping on epoch 5)\n",
            "\t0.9172\t = Validation score   (roc_auc)\n",
            "\t166.79s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 22.94s of the 22.93s of remaining time.\n",
            "\tFitting with cpus=4, gpus=0, mem=6.8/36.7 GB\n",
            "\t0.8771\t = Validation score   (roc_auc)\n",
            "\t22.98s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -0.15s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM': 0.6, 'LightGBMXT': 0.36, 'NeuralNetFastAI': 0.04}\n",
            "\t0.9719\t = Validation score   (roc_auc)\n",
            "\t0.16s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 1802.4s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2103.6 rows/s (5906 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/ag_models\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val eval_metric  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.971892     roc_auc       2.807516  1005.767153                0.001033           0.160867            2       True          9\n",
            "1             LightGBM   0.970669     roc_auc       0.764521   361.411730                0.764521         361.411730            1       True          2\n",
            "2           LightGBMXT   0.969097     roc_auc       1.827361   477.403704                1.827361         477.403704            1       True          1\n",
            "3     RandomForestEntr   0.936494     roc_auc       0.172647   185.789573                0.172647         185.789573            1       True          4\n",
            "4     RandomForestGini   0.934371     roc_auc       0.151000   236.999153                0.151000         236.999153            1       True          3\n",
            "5       ExtraTreesEntr   0.921552     roc_auc       0.171492   162.362179                0.171492         162.362179            1       True          6\n",
            "6      NeuralNetFastAI   0.917193     roc_auc       0.214602   166.790852                0.214602         166.790852            1       True          7\n",
            "7       ExtraTreesGini   0.904073     roc_auc       0.159775   160.705393                0.159775         160.705393            1       True          5\n",
            "8              XGBoost   0.877148     roc_auc       0.091559    22.979182                0.091559          22.979182            1       True          8\n",
            "Number of models trained: 9\n",
            "Types of models trained:\n",
            "{'RFModel', 'XTModel', 'XGBoostModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel', 'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "('float', [])    : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "('int', [])      :   2 | ['TransactionDT', 'card1']\n",
            "Plot summary of models saved to file: /content/ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'LightGBMXT': 'LGBModel', 'LightGBM': 'LGBModel', 'RandomForestGini': 'RFModel', 'RandomForestEntr': 'RFModel', 'ExtraTreesGini': 'XTModel', 'ExtraTreesEntr': 'XTModel', 'NeuralNetFastAI': 'NNFastAiTabularModel', 'XGBoost': 'XGBoostModel', 'WeightedEnsemble_L2': 'WeightedEnsembleModel'}, 'model_performance': {'LightGBMXT': np.float64(0.9690970447396059), 'LightGBM': np.float64(0.9706686400614397), 'RandomForestGini': np.float64(0.9343710609455171), 'RandomForestEntr': np.float64(0.9364944947541436), 'ExtraTreesGini': np.float64(0.904072500218277), 'ExtraTreesEntr': np.float64(0.9215524717023836), 'NeuralNetFastAI': np.float64(0.9171928628889041), 'XGBoost': np.float64(0.8771481224352438), 'WeightedEnsemble_L2': np.float64(0.9718918396565887)}, 'model_best': 'WeightedEnsemble_L2', 'model_paths': {'LightGBMXT': ['LightGBMXT'], 'LightGBM': ['LightGBM'], 'RandomForestGini': ['RandomForestGini'], 'RandomForestEntr': ['RandomForestEntr'], 'ExtraTreesGini': ['ExtraTreesGini'], 'ExtraTreesEntr': ['ExtraTreesEntr'], 'NeuralNetFastAI': ['NeuralNetFastAI'], 'XGBoost': ['XGBoost'], 'WeightedEnsemble_L2': ['WeightedEnsemble_L2']}, 'model_fit_times': {'LightGBMXT': 477.4037039279938, 'LightGBM': 361.41173028945923, 'RandomForestGini': 236.99915266036987, 'RandomForestEntr': 185.78957271575928, 'ExtraTreesGini': 160.705393075943, 'ExtraTreesEntr': 162.36217880249023, 'NeuralNetFastAI': 166.79085183143616, 'XGBoost': 22.97918152809143, 'WeightedEnsemble_L2': 0.16086697578430176}, 'model_pred_times': {'LightGBMXT': 1.8273608684539795, 'LightGBM': 0.7645211219787598, 'RandomForestGini': 0.1509995460510254, 'RandomForestEntr': 0.17264652252197266, 'ExtraTreesGini': 0.1597750186920166, 'ExtraTreesEntr': 0.1714920997619629, 'NeuralNetFastAI': 0.2146015167236328, 'XGBoost': 0.09155917167663574, 'WeightedEnsemble_L2': 0.0010328292846679688}, 'num_bag_folds': 0, 'max_stack_level': 2, 'num_classes': 2, 'model_hyperparams': {'LightGBMXT': {'learning_rate': 0.05, 'extra_trees': True}, 'LightGBM': {'learning_rate': 0.05}, 'RandomForestGini': {'n_estimators': 300, 'max_leaf_nodes': 15000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'criterion': 'gini'}, 'RandomForestEntr': {'n_estimators': 300, 'max_leaf_nodes': 15000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'criterion': 'entropy'}, 'ExtraTreesGini': {'n_estimators': 300, 'max_leaf_nodes': 15000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'criterion': 'gini'}, 'ExtraTreesEntr': {'n_estimators': 300, 'max_leaf_nodes': 15000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'criterion': 'entropy'}, 'NeuralNetFastAI': {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}, 'XGBoost': {'n_estimators': 10000, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'binary:logistic', 'booster': 'gbtree'}, 'WeightedEnsemble_L2': {'use_orig_features': False, 'valid_stacker': True, 'max_base_models': 0, 'max_base_models_per_type': 'auto', 'save_bag_folds': True, 'stratify': 'auto', 'bin': 'auto', 'n_bins': None}}, 'leaderboard':                  model  score_val eval_metric  pred_time_val     fit_time  \\\n",
            "0  WeightedEnsemble_L2   0.971892     roc_auc       2.807516  1005.767153   \n",
            "1             LightGBM   0.970669     roc_auc       0.764521   361.411730   \n",
            "2           LightGBMXT   0.969097     roc_auc       1.827361   477.403704   \n",
            "3     RandomForestEntr   0.936494     roc_auc       0.172647   185.789573   \n",
            "4     RandomForestGini   0.934371     roc_auc       0.151000   236.999153   \n",
            "5       ExtraTreesEntr   0.921552     roc_auc       0.171492   162.362179   \n",
            "6      NeuralNetFastAI   0.917193     roc_auc       0.214602   166.790852   \n",
            "7       ExtraTreesGini   0.904073     roc_auc       0.159775   160.705393   \n",
            "8              XGBoost   0.877148     roc_auc       0.091559    22.979182   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.001033           0.160867            2       True   \n",
            "1                0.764521         361.411730            1       True   \n",
            "2                1.827361         477.403704            1       True   \n",
            "3                0.172647         185.789573            1       True   \n",
            "4                0.151000         236.999153            1       True   \n",
            "5                0.171492         162.362179            1       True   \n",
            "6                0.214602         166.790852            1       True   \n",
            "7                0.159775         160.705393            1       True   \n",
            "8                0.091559          22.979182            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          9  \n",
            "1          2  \n",
            "2          1  \n",
            "3          4  \n",
            "4          3  \n",
            "5          6  \n",
            "6          7  \n",
            "7          5  \n",
            "8          8  }\n"
          ]
        }
      ],
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "label = 'isFraud'\n",
        "if label not in train.columns:\n",
        "    raise SystemExit(f'Label {label} not in train')\n",
        "cols_to_drop = ['TransactionID']\n",
        "train_features = train.drop(columns=[c for c in cols_to_drop if c in train.columns])\n",
        "test_features = test.drop(columns=[c for c in cols_to_drop if c in test.columns])\n",
        "save_path = '/content/ag_models'\n",
        "predictor = TabularPredictor(label=label, path=save_path, eval_metric='roc_auc').fit(train_features, presets='medium_quality', time_limit=1800)\n",
        "print(predictor.fit_summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b6ef67d",
      "metadata": {
        "id": "4b6ef67d"
      },
      "source": [
        "## 7 — Predict & prepare submission\n",
        "We predict class probabilities and write the `submission.csv` expected by the competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "168c46cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "168c46cb",
        "outputId": "30d9d9da-60d9-45e1-f1a3-044b69ad4830"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"38 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 38 missing columns: ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'] | 432 available columns: ['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'id-01', 'id-02', 'id-03', 'id-04', 'id-05', 'id-06', 'id-07', 'id-08', 'id-09', 'id-10', 'id-11', 'id-12', 'id-13', 'id-14', 'id-15', 'id-16', 'id-17', 'id-18', 'id-19', 'id-20', 'id-21', 'id-22', 'id-23', 'id-24', 'id-25', 'id-26', 'id-27', 'id-28', 'id-29', 'id-30', 'id-31', 'id-32', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38', 'DeviceType', 'DeviceInfo']\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/features/generators/abstract.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;31m# therefore, try avoid copying by checking the expected features first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'] not in index\"",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1228899122.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# pick positive class probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, data, model, as_pandas, as_multiclass, transform_features)\u001b[0m\n\u001b[1;32m   2487\u001b[0m             )\n\u001b[1;32m   2488\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_pandas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_multiclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_multiclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_from_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_threshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/tabular/learner/abstract_learner.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, model, as_pandas, as_multiclass, inverse_transform, transform_features)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0my_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         y_pred_proba = self._post_process_predict_proba(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/tabular/learner/abstract_learner.py\u001b[0m in \u001b[0;36mtransform_features\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature_generator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_generators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/features/generators/abstract.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mmissing_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0;34mf\"{len(missing_cols)} required columns are missing from the provided dataset to transform using {self.__class__.__name__}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;34mf\"{len(missing_cols)} missing columns: {missing_cols} | \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"38 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 38 missing columns: ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'] | 432 available columns: ['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', '..."
          ]
        }
      ],
      "source": [
        "proba = predictor.predict_proba(test_features)\n",
        "import pandas as pd\n",
        "if isinstance(proba, pd.DataFrame):\n",
        "    # pick positive class probability\n",
        "    if 1 in proba.columns:\n",
        "        preds = proba[1]\n",
        "    else:\n",
        "        preds = proba.iloc[:, -1]\n",
        "else:\n",
        "    preds = proba\n",
        "submission = pd.read_csv(paths['sample_submission'])\n",
        "submission['isFraud'] = preds\n",
        "submission.to_csv('/content/submission.csv', index=False)\n",
        "print('Saved /content/submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624bb69a",
      "metadata": {
        "id": "624bb69a"
      },
      "source": [
        "## 8 — Submit to Kaggle (optional)\n",
        "You can submit via the Kaggle CLI. If you prefer, download `/content/submission.csv` and upload manually on the competition page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ec0be0",
      "metadata": {
        "id": "45ec0be0"
      },
      "outputs": [],
      "source": [
        "# Kaggle submit (uncomment to run)\n",
        "# competition = 'ieee-fraud-detection'\n",
        "!kaggle competitions submit -c $competition -f /content/submission.csv -m 'AutoGluon colab submission'\n",
        "# print('To submit: run kaggle competitions submit -c ieee-fraud-detection -f /content/submission.csv -m ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ddb73e8",
      "metadata": {
        "id": "9ddb73e8"
      },
      "source": [
        "---\n",
        "### Notes:\n",
        "- Keep `kaggle.json` private.\n",
        "- For reproducibility, save `/content/ag_models` to Drive after training.\n",
        "- If Colab runs out of RAM, try a smaller sample or use a larger VM."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}