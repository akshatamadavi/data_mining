{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSQKur7zOzbN0rMRELA8G2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatamadavi/data_mining/blob/main/clustering/04_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr1OYoVxyJQB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part D: DBSCAN Clustering using PyCaret\n",
        "\n",
        "This notebook demonstrates DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering using the PyCaret library. DBSCAN is a density-based clustering algorithm that can discover clusters of arbitrary shapes and identify outliers as noise points.\n",
        "\n",
        "**Key Features of DBSCAN:**\n",
        "- Does not require specifying the number of clusters beforehand\n",
        "- Can identify outliers/noise in the data\n",
        "- Works well with clusters of varying shapes and densities\n",
        "- Requires two parameters: eps (neighborhood radius) and min_samples (minimum points to form a cluster)"
      ],
      "metadata": {
        "id": "Rcq0Tj4Wzi5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyCaret\n",
        "!pip install pycaret --quiet\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from pycaret.clustering import *\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('✓ All libraries imported successfully!')"
      ],
      "metadata": {
        "id": "7tseK1R0z-dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load and Prepare Dataset\n",
        "\n",
        "We'll create a synthetic dataset with non-linear clusters (moons shape) to demonstrate DBSCAN's ability to identify complex cluster shapes."
      ],
      "metadata": {
        "id": "ZZ6EFNQC0UBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic dataset with non-linear clusters (moons shape)\n",
        "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Visualize the raw data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['Feature_1'], df['Feature_2'], c='gray', alpha=0.6, edgecolors='black', s=50)\n",
        "plt.title('Original Dataset (Moons Shape) - Before Clustering', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Dataset created and visualized successfully!\")"
      ],
      "metadata": {
        "id": "PUD1ocOy1B_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setup PyCaret Clustering Environment\n",
        "\n",
        "Initialize PyCaret's clustering environment with the dataset. PyCaret will automatically preprocess the data and prepare it for clustering."
      ],
      "metadata": {
        "id": "YB69IxLh1HUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret clustering environment\n",
        "# This step preprocesses the data and sets up the clustering pipeline\n",
        "cluster_setup = setup(data=df, session_id=42, verbose=False)\n",
        "\n",
        "print(\"✓ PyCaret clustering environment initialized successfully!\")"
      ],
      "metadata": {
        "id": "Gxtz9tgD1f_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create DBSCAN Clustering Model\n",
        "\n",
        "Now we'll create a DBSCAN model using PyCaret's `create_model` function. DBSCAN will automatically identify clusters based on density."
      ],
      "metadata": {
        "id": "QDw73nUl1l-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DBSCAN model using PyCaret\n",
        "dbscan_model = create_model('dbscan')\n",
        "\n",
        "print(\"✓ DBSCAN model created successfully!\")\n",
        "print(f\"\\nModel Details:\")\n",
        "print(f\"Number of clusters found: {len(set(dbscan_model.labels_)) - (1 if -1 in dbscan_model.labels_ else 0)}\")\n",
        "print(f\"Number of noise points: {list(dbscan_model.labels_).count(-1)}\")\n",
        "print(f\"eps (neighborhood radius): {dbscan_model.eps}\")\n",
        "print(f\"min_samples: {dbscan_model.min_samples}\")"
      ],
      "metadata": {
        "id": "GJCduOQi1xuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualize Clustering Results and Evaluate Quality\n",
        "\n",
        "Visualize the DBSCAN clustering results and evaluate the clustering quality using various metrics."
      ],
      "metadata": {
        "id": "Pmrv5CUt13fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cluster predictions\n",
        "clusters = assign_model(dbscan_model)\n",
        "\n",
        "# Visualize clustering using PyCaret's plot_model\n",
        "print(\"Generating clustering visualizations...\\n\")\n",
        "plot_model(dbscan_model)\n",
        "\n",
        "# Custom visualization with matplotlib\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Clusters with colors\n",
        "for cluster_id in set(dbscan_model.labels_):\n",
        "    if cluster_id == -1:\n",
        "        # Noise points in black\n",
        "        cluster_mask = dbscan_model.labels_ == cluster_id\n",
        "        axes[0].scatter(df[cluster_mask]['Feature_1'], df[cluster_mask]['Feature_2'],\n",
        "                       c='black', marker='x', s=50, alpha=0.5, label='Noise')\n",
        "    else:\n",
        "        cluster_mask = dbscan_model.labels_ == cluster_id\n",
        "        axes[0].scatter(df[cluster_mask]['Feature_1'], df[cluster_mask]['Feature_2'],\n",
        "                       s=100, alpha=0.6, edgecolors='black', label=f'Cluster {cluster_id}')\n",
        "\n",
        "axes[0].set_title('DBSCAN Clustering Results', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Cluster sizes\n",
        "cluster_counts = pd.Series(dbscan_model.labels_).value_counts().sort_index()\n",
        "colors = ['black' if x == -1 else f'C{x}' for x in cluster_counts.index]\n",
        "axes[1].bar(range(len(cluster_counts)), cluster_counts.values, color=colors, edgecolor='black')\n",
        "axes[1].set_title('Cluster Sizes', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Cluster ID (-1 = Noise)')\n",
        "axes[1].set_ylabel('Number of Points')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate clustering quality\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "# Filter out noise points for metrics\n",
        "mask = dbscan_model.labels_ != -1\n",
        "if mask.sum() > 0:\n",
        "    silhouette_avg = silhouette_score(df[mask], dbscan_model.labels_[mask])\n",
        "    db_score = davies_bouldin_score(df[mask], dbscan_model.labels_[mask])\n",
        "    ch_score = calinski_harabasz_score(df[mask], dbscan_model.labels_[mask])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLUSTERING QUALITY METRICS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Silhouette Score: {silhouette_avg:.4f} (higher is better, range: [-1, 1])\")\n",
        "    print(f\"Davies-Bouldin Index: {db_score:.4f} (lower is better)\")\n",
        "    print(f\"Calinski-Harabasz Score: {ch_score:.4f} (higher is better)\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\nNo valid clusters found for evaluation.\")\n",
        "\n",
        "print(\"\\n✓ Visualization and evaluation completed successfully!\")"
      ],
      "metadata": {
        "id": "EglDhrvh2N0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "This notebook successfully demonstrated DBSCAN clustering using PyCaret library. Key takeaways:\n",
        "\n",
        "**Advantages of DBSCAN:**\n",
        "- Automatically discovers the number of clusters\n",
        "- Identifies outliers/noise points effectively\n",
        "- Works well with non-linear cluster shapes (as shown with moons dataset)\n",
        "- No need to specify cluster count beforehand\n",
        "\n",
        "**PyCaret Benefits:**\n",
        "- Simple and intuitive API for clustering tasks\n",
        "- Automated preprocessing and setup\n",
        "- Built-in visualization and evaluation tools\n",
        "- Reduces code complexity significantly\n",
        "\n",
        "**Clustering Quality:**\n",
        "The evaluation metrics (Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Score) provide quantitative measures of how well-separated and compact the clusters are.\n",
        "\n",
        "For production use, consider tuning the `eps` and `min_samples` parameters based on your specific dataset characteristics."
      ],
      "metadata": {
        "id": "rJdzJsx62WpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install and Import Required Libraries\n",
        "\n",
        "First, we need to install PyCaret library and import necessary packages for data manipulation and visualization."
      ],
      "metadata": {
        "id": "zBnz7-iN0Aun"
      }
    }
  ]
}