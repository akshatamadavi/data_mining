{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatamadavi/data_mining/blob/main/unsloth_ai/05_continued_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3SebxhaCtiP"
      },
      "source": [
        "# 05_continued_pretraining.ipynb ‚Äî Unsloth Continued Pretraining (Causal LM)\n",
        "\n",
        "**Goal:** Continue pretraining a small Unsloth model on **unlabeled text** (new language/domain) using a **causal language modeling** objective.\n",
        "\n",
        "**What this notebook covers**\n",
        "1. Environment & GPU check\n",
        "2. Install dependencies\n",
        "3. Load a tiny Unsloth model in **4-bit** and attach **LoRA** adapters for efficient updates\n",
        "4. Build a small corpus (or load your own `.txt` files / JSONL)\n",
        "5. Tokenize into contiguous blocks and train with `Trainer` (`mlm=False`)\n",
        "6. Save checkpoint + quick inference demo\n",
        "\n",
        "> Replace the toy corpus with your real data to teach the model a **new language or domain**."
      ],
      "id": "Y3SebxhaCtiP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UAUzxIaUCtiR"
      },
      "outputs": [],
      "source": [
        "#@title ‚è±Ô∏è Setup ‚Äî GPU check\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Running on CPU ‚Äî switch Colab runtime to GPU for training speed.\")"
      ],
      "id": "UAUzxIaUCtiR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dOutwKkICtiS"
      },
      "outputs": [],
      "source": [
        "#@title üì¶ Install libraries (Colab-friendly)\n",
        "!pip -q install -U unsloth transformers datasets bitsandbytes peft accelerate sentencepiece\n",
        "import os\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\""
      ],
      "id": "dOutwKkICtiS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZMXxfzjiCtiS"
      },
      "outputs": [],
      "source": [
        "#@title üîß Config ‚Äî model & training params\n",
        "from dataclasses import dataclass\n",
        "\n",
        "BASE_MODEL = \"unsloth/SmolLM2-135M-Instruct-bnb-4bit\"  # tiny + fast; swap for Gemma/Llama/Mistral if you have VRAM\n",
        "OUTPUT_DIR = \"outputs_continued_pretrain\"\n",
        "\n",
        "# Training\n",
        "MAX_SEQ_LEN = 1024\n",
        "BATCH_PER_DEVICE = 4\n",
        "GRAD_ACCUM = 8\n",
        "EPOCHS = 2  # increase for real training\n",
        "LR = 2e-5\n",
        "SEED = 42\n",
        "\n",
        "print({k:v for k,v in dict(BASE_MODEL=BASE_MODEL, OUTPUT_DIR=OUTPUT_DIR, MAX_SEQ_LEN=MAX_SEQ_LEN,\n",
        "                           BATCH_PER_DEVICE=BATCH_PER_DEVICE, GRAD_ACCUM=GRAD_ACCUM,\n",
        "                           EPOCHS=EPOCHS, LR=LR, SEED=SEED).items()})"
      ],
      "id": "ZMXxfzjiCtiS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv7oqsk8CtiT"
      },
      "source": [
        "## 1) Load model in 4-bit and attach LoRA for continued pretraining\n",
        "- LoRA adapters update a small subset of parameters (efficient)\n",
        "- 4-bit quantization keeps VRAM low\n",
        "- We train with **causal LM** objective (next-token prediction)"
      ],
      "id": "mv7oqsk8CtiT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s4s1dyvaCtiT"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from transformers import AutoConfig\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = SEED,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        ")\n",
        "print(\"Model & tokenizer ready.\")"
      ],
      "id": "s4s1dyvaCtiT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J5lLNtiCtiT"
      },
      "source": [
        "## 2) Build / load your corpus\n",
        "You can provide a folder of `.txt` files or a JSONL of `{\"text\": ...}` rows.\n",
        "\n",
        "Below we create a **toy corpus** with a few sentences in a hypothetical new language/domain. Replace with your own."
      ],
      "id": "2J5lLNtiCtiT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KVVfZDgoCtiT"
      },
      "outputs": [],
      "source": [
        "import os, json, glob\n",
        "from datasets import Dataset\n",
        "\n",
        "#@title üëâ Choose your corpus source\n",
        "USE_TXT_FOLDER = False #@param {type:\"boolean\"}\n",
        "TXT_FOLDER = \"/content/corpus_txt\" #@param {type:\"string\"}\n",
        "JSONL_PATH = \"/content/corpus.jsonl\" #@param {type:\"string\"}\n",
        "\n",
        "def ensure_toy_corpus():\n",
        "    toy = [\n",
        "        {\"text\": \"Nolori safi tem. Vairu melek tora; kivar duneh. (NewLang)\"},\n",
        "        {\"text\": \"Data sciencia praxi: version control, reproducibilis, experimentum tracking.\"},\n",
        "        {\"text\": \"Guidelines: tokens segmente, contextus longus, regulae syntaxicae novas.\"},\n",
        "        {\"text\": \"Conversatio: Q: 'Salve?' A: 'Pax et lumen!'\"},\n",
        "        {\"text\": \"Domaino-medicus: symptomata, anamnesis, differentialis, consilium therapiae.\"}\n",
        "    ]\n",
        "    with open(JSONL_PATH, \"w\") as f:\n",
        "        for r in toy:\n",
        "            f.write(json.dumps(r, ensure_ascii=False)+\"\\n\")\n",
        "\n",
        "def load_corpus_dataset():\n",
        "    if USE_TXT_FOLDER and os.path.isdir(TXT_FOLDER):\n",
        "        texts = []\n",
        "        for p in glob.glob(os.path.join(TXT_FOLDER, \"**/*.txt\"), recursive=True):\n",
        "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                txt = f.read().strip()\n",
        "                if txt:\n",
        "                    texts.append({\"text\": txt})\n",
        "        if not texts:\n",
        "            ensure_toy_corpus()\n",
        "            print(\"No .txt files found; using toy corpus.\")\n",
        "            return Dataset.from_json(JSONL_PATH)\n",
        "        return Dataset.from_list(texts)\n",
        "    else:\n",
        "        if not os.path.exists(JSONL_PATH):\n",
        "            ensure_toy_corpus()\n",
        "            print(\"Created toy JSONL corpus at\", JSONL_PATH)\n",
        "        return Dataset.from_json(JSONL_PATH)\n",
        "\n",
        "train_raw = load_corpus_dataset()\n",
        "train_raw"
      ],
      "id": "KVVfZDgoCtiT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQvYJjX4CtiT"
      },
      "source": [
        "## 3) Tokenize into contiguous blocks for CLM\n",
        "- We **concatenate** texts and split into blocks of `MAX_SEQ_LEN` tokens\n",
        "- Use `DataCollatorForLanguageModeling` with `mlm=False` (causal LM)"
      ],
      "id": "MQvYJjX4CtiT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VvmdEltoCtiT"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "import itertools\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"text\"], add_special_tokens=False)\n",
        "\n",
        "tokenized = train_raw.map(tokenize_function, batched=True, remove_columns=[c for c in train_raw.column_names if c != \"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate\n",
        "    concatenated = list(itertools.chain.from_iterable(examples[\"input_ids\"]))\n",
        "    total_length = (len(concatenated) // MAX_SEQ_LEN) * MAX_SEQ_LEN\n",
        "    concatenated = concatenated[:total_length]\n",
        "    # Split\n",
        "    result = {\n",
        "        \"input_ids\": [concatenated[i:i+MAX_SEQ_LEN] for i in range(0, total_length, MAX_SEQ_LEN)]\n",
        "    }\n",
        "    result[\"labels\"] = [ids.copy() for ids in result[\"input_ids\"]]\n",
        "    return result\n",
        "\n",
        "lm_ds = tokenized.map(group_texts, batched=True, remove_columns=tokenized.column_names)\n",
        "lm_ds"
      ],
      "id": "VvmdEltoCtiT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2-pC4w-6CtiU"
      },
      "outputs": [],
      "source": [
        "#@title 4) Train with Trainer (causal LM)\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir = OUTPUT_DIR,\n",
        "    per_device_train_batch_size = BATCH_PER_DEVICE,\n",
        "    gradient_accumulation_steps = GRAD_ACCUM,\n",
        "    num_train_epochs = EPOCHS,\n",
        "    learning_rate = LR,\n",
        "    warmup_ratio = 0.1,\n",
        "    logging_steps = 10,\n",
        "    save_steps = 200,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    optim = \"adamw_8bit\",\n",
        "    seed = SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = args,\n",
        "    train_dataset = lm_ds,\n",
        "    data_collator = data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved to:\", OUTPUT_DIR)"
      ],
      "id": "2-pC4w-6CtiU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EeWC8r9CtiU"
      },
      "source": [
        "## 5) Quick inference sanity check\n",
        "Generate text in the **new language/domain** to see if the model picked up patterns."
      ],
      "id": "8EeWC8r9CtiU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ME3UwYPKCtiU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate(prompt, max_new_tokens=120, temperature=0.7, top_p=0.9):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True,\n",
        "                             temperature=temperature, top_p=top_p)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompt = \"Nolori safi tem:\"\n",
        "print(\"\\n=== Sample Generation ===\\n\")\n",
        "print(generate(test_prompt))"
      ],
      "id": "ME3UwYPKCtiU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJLOrJfhCtiU"
      },
      "source": [
        "## 6) (Optional) Export to Ollama (manual Modelfile)\n",
        "Ollama can load HF adapters via a **Modelfile**. The simplest path is to export the PEFT adapters and point Ollama to the base model + LoRA.\n",
        "\n",
        "Below we just create a skeleton `Modelfile`. You may need to adjust paths on your machine.\n"
      ],
      "id": "uJLOrJfhCtiU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QuuiXsSaCtiU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "modelfile_path = Path(OUTPUT_DIR)/\"Modelfile\"\n",
        "modelfile_text = f\"\"\"\n",
        "# Example Modelfile for Ollama (adjust paths on your system)\n",
        "FROM {BASE_MODEL}\n",
        "PARAMETER lora {OUTPUT_DIR}\n",
        "TEMPLATE \"You are a helpful assistant.\"\n",
        "\"\"\"\n",
        "modelfile_path.write_text(modelfile_text)\n",
        "print(\"Wrote Modelfile to:\", modelfile_path)\n",
        "print(\"Next on your machine:  ollama create my-continued-model -f\", modelfile_path)\n"
      ],
      "id": "QuuiXsSaCtiU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gWwOyIfCtiV"
      },
      "source": [
        "---\n",
        "### Notes & Tips\n",
        "- For **larger corpora**, increase `EPOCHS` and adjust batch/grad-accum. If you hit OOM, reduce `BATCH_PER_DEVICE` or `MAX_SEQ_LEN`.\n",
        "- To use a different base (e.g., `unsloth/gemma-3-1b-it-bnb-4bit`), just change `BASE_MODEL` and re-run.\n",
        "- If your tokenizer needs special handling (e.g., custom BOS/EOS), set `tokenizer.pad_token = tokenizer.eos_token` (already handled above).\n",
        "- Keep text **clean & UTF-8**. For multilingual corpora, ensure the base model supports that script/tokenization reasonably well.\n",
        "- For full finetuning (no LoRA), load in higher precision and skip `get_peft_model` (requires more VRAM).\n",
        "\n",
        "Happy continued pretraining! ü¶•"
      ],
      "id": "8gWwOyIfCtiV"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}