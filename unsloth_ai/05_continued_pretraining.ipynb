{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatamadavi/data_mining/blob/main/unsloth_ai/05_continued_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3SebxhaCtiP"
      },
      "source": [
        "# 05_continued_pretraining.ipynb â€” Unsloth Continued Pretraining (Causal LM)\n",
        "\n",
        "**Goal:** Continue pretraining a small Unsloth model on **unlabeled text** (new language/domain) using a **causal language modeling** objective.\n",
        "\n",
        "**What this notebook covers**\n",
        "1. Environment & GPU check\n",
        "2. Install dependencies\n",
        "3. Load a tiny Unsloth model in **4-bit** and attach **LoRA** adapters for efficient updates\n",
        "4. Build a small corpus (or load your own `.txt` files / JSONL)\n",
        "5. Tokenize into contiguous blocks and train with `Trainer` (`mlm=False`)\n",
        "6. Save checkpoint + quick inference demo\n",
        "\n",
        "> Replace the toy corpus with your real data to teach the model a **new language or domain**."
      ],
      "id": "Y3SebxhaCtiP"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UAUzxIaUCtiR",
        "outputId": "939ac7c2-a452-4e9d-cb28-3686a42eb93d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "CUDA available: False\n",
            "Running on CPU â€” switch Colab runtime to GPU for training speed.\n"
          ]
        }
      ],
      "source": [
        "#@title â±ï¸ Setup â€” GPU check\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Running on CPU â€” switch Colab runtime to GPU for training speed.\")"
      ],
      "id": "UAUzxIaUCtiR"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dOutwKkICtiS",
        "outputId": "1191fb8a-ece5-4c32-8eb2-8981c7d25c62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title ğŸ“¦ Install libraries (Colab-friendly)\n",
        "!pip -q install -U unsloth transformers datasets bitsandbytes peft accelerate sentencepiece\n",
        "import os\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\""
      ],
      "id": "dOutwKkICtiS"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZMXxfzjiCtiS",
        "outputId": "075ec800-8b20-4b18-ab45-3d4284407566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'BASE_MODEL': 'unsloth/SmolLM2-135M-Instruct-bnb-4bit', 'OUTPUT_DIR': 'outputs_continued_pretrain', 'MAX_SEQ_LEN': 1024, 'BATCH_PER_DEVICE': 4, 'GRAD_ACCUM': 8, 'EPOCHS': 2, 'LR': 2e-05, 'SEED': 42}\n"
          ]
        }
      ],
      "source": [
        "#@title ğŸ”§ Config â€” model & training params\n",
        "from dataclasses import dataclass\n",
        "\n",
        "BASE_MODEL = \"unsloth/SmolLM2-135M-Instruct-bnb-4bit\"  # tiny + fast; swap for Gemma/Llama/Mistral if you have VRAM\n",
        "OUTPUT_DIR = \"outputs_continued_pretrain\"\n",
        "\n",
        "# Training\n",
        "MAX_SEQ_LEN = 1024\n",
        "BATCH_PER_DEVICE = 4\n",
        "GRAD_ACCUM = 8\n",
        "EPOCHS = 2  # increase for real training\n",
        "LR = 2e-5\n",
        "SEED = 42\n",
        "\n",
        "print({k:v for k,v in dict(BASE_MODEL=BASE_MODEL, OUTPUT_DIR=OUTPUT_DIR, MAX_SEQ_LEN=MAX_SEQ_LEN,\n",
        "                           BATCH_PER_DEVICE=BATCH_PER_DEVICE, GRAD_ACCUM=GRAD_ACCUM,\n",
        "                           EPOCHS=EPOCHS, LR=LR, SEED=SEED).items()})"
      ],
      "id": "ZMXxfzjiCtiS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv7oqsk8CtiT"
      },
      "source": [
        "## 1) Load model in 4-bit and attach LoRA for continued pretraining\n",
        "- LoRA adapters update a small subset of parameters (efficient)\n",
        "- 4-bit quantization keeps VRAM low\n",
        "- We train with **causal LM** objective (next-token prediction)"
      ],
      "id": "mv7oqsk8CtiT"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "s4s1dyvaCtiT",
        "outputId": "fd9c25cc-41f4-4627-80ee-4b33edd8117d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth cannot find any torch accelerator? You need a GPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-581808652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_bfloat16_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBASE_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#         except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     raise ImportError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# Get device types and other variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m from .device_type import (\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mis_hip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mget_device_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/device_type.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA, AMD and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# HIP fails for autocast and other torch functions. Use CUDA instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mDEVICE_TYPE_TORCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/device_type.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accelerator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth cannot find any torch accelerator? You need a GPU.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0maccelerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccelerator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth cannot find any torch accelerator? You need a GPU."
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from transformers import AutoConfig\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = SEED,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        ")\n",
        "print(\"Model & tokenizer ready.\")"
      ],
      "id": "s4s1dyvaCtiT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J5lLNtiCtiT"
      },
      "source": [
        "## 2) Build / load your corpus\n",
        "You can provide a folder of `.txt` files or a JSONL of `{\"text\": ...}` rows.\n",
        "\n",
        "Below we create a **toy corpus** with a few sentences in a hypothetical new language/domain. Replace with your own."
      ],
      "id": "2J5lLNtiCtiT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KVVfZDgoCtiT"
      },
      "outputs": [],
      "source": [
        "import os, json, glob\n",
        "from datasets import Dataset\n",
        "\n",
        "#@title ğŸ‘‰ Choose your corpus source\n",
        "USE_TXT_FOLDER = False #@param {type:\"boolean\"}\n",
        "TXT_FOLDER = \"/content/corpus_txt\" #@param {type:\"string\"}\n",
        "JSONL_PATH = \"/content/corpus.jsonl\" #@param {type:\"string\"}\n",
        "\n",
        "def ensure_toy_corpus():\n",
        "    toy = [\n",
        "        {\"text\": \"Nolori safi tem. Vairu melek tora; kivar duneh. (NewLang)\"},\n",
        "        {\"text\": \"Data sciencia praxi: version control, reproducibilis, experimentum tracking.\"},\n",
        "        {\"text\": \"Guidelines: tokens segmente, contextus longus, regulae syntaxicae novas.\"},\n",
        "        {\"text\": \"Conversatio: Q: 'Salve?' A: 'Pax et lumen!'\"},\n",
        "        {\"text\": \"Domaino-medicus: symptomata, anamnesis, differentialis, consilium therapiae.\"}\n",
        "    ]\n",
        "    with open(JSONL_PATH, \"w\") as f:\n",
        "        for r in toy:\n",
        "            f.write(json.dumps(r, ensure_ascii=False)+\"\\n\")\n",
        "\n",
        "def load_corpus_dataset():\n",
        "    if USE_TXT_FOLDER and os.path.isdir(TXT_FOLDER):\n",
        "        texts = []\n",
        "        for p in glob.glob(os.path.join(TXT_FOLDER, \"**/*.txt\"), recursive=True):\n",
        "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                txt = f.read().strip()\n",
        "                if txt:\n",
        "                    texts.append({\"text\": txt})\n",
        "        if not texts:\n",
        "            ensure_toy_corpus()\n",
        "            print(\"No .txt files found; using toy corpus.\")\n",
        "            return Dataset.from_json(JSONL_PATH)\n",
        "        return Dataset.from_list(texts)\n",
        "    else:\n",
        "        if not os.path.exists(JSONL_PATH):\n",
        "            ensure_toy_corpus()\n",
        "            print(\"Created toy JSONL corpus at\", JSONL_PATH)\n",
        "        return Dataset.from_json(JSONL_PATH)\n",
        "\n",
        "train_raw = load_corpus_dataset()\n",
        "train_raw"
      ],
      "id": "KVVfZDgoCtiT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQvYJjX4CtiT"
      },
      "source": [
        "## 3) Tokenize into contiguous blocks for CLM\n",
        "- We **concatenate** texts and split into blocks of `MAX_SEQ_LEN` tokens\n",
        "- Use `DataCollatorForLanguageModeling` with `mlm=False` (causal LM)"
      ],
      "id": "MQvYJjX4CtiT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VvmdEltoCtiT"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "import itertools\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"text\"], add_special_tokens=False)\n",
        "\n",
        "tokenized = train_raw.map(tokenize_function, batched=True, remove_columns=[c for c in train_raw.column_names if c != \"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate\n",
        "    concatenated = list(itertools.chain.from_iterable(examples[\"input_ids\"]))\n",
        "    total_length = (len(concatenated) // MAX_SEQ_LEN) * MAX_SEQ_LEN\n",
        "    concatenated = concatenated[:total_length]\n",
        "    # Split\n",
        "    result = {\n",
        "        \"input_ids\": [concatenated[i:i+MAX_SEQ_LEN] for i in range(0, total_length, MAX_SEQ_LEN)]\n",
        "    }\n",
        "    result[\"labels\"] = [ids.copy() for ids in result[\"input_ids\"]]\n",
        "    return result\n",
        "\n",
        "lm_ds = tokenized.map(group_texts, batched=True, remove_columns=tokenized.column_names)\n",
        "lm_ds"
      ],
      "id": "VvmdEltoCtiT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2-pC4w-6CtiU"
      },
      "outputs": [],
      "source": [
        "#@title 4) Train with Trainer (causal LM)\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir = OUTPUT_DIR,\n",
        "    per_device_train_batch_size = BATCH_PER_DEVICE,\n",
        "    gradient_accumulation_steps = GRAD_ACCUM,\n",
        "    num_train_epochs = EPOCHS,\n",
        "    learning_rate = LR,\n",
        "    warmup_ratio = 0.1,\n",
        "    logging_steps = 10,\n",
        "    save_steps = 200,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    optim = \"adamw_8bit\",\n",
        "    seed = SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = args,\n",
        "    train_dataset = lm_ds,\n",
        "    data_collator = data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved to:\", OUTPUT_DIR)"
      ],
      "id": "2-pC4w-6CtiU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EeWC8r9CtiU"
      },
      "source": [
        "## 5) Quick inference sanity check\n",
        "Generate text in the **new language/domain** to see if the model picked up patterns."
      ],
      "id": "8EeWC8r9CtiU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ME3UwYPKCtiU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate(prompt, max_new_tokens=120, temperature=0.7, top_p=0.9):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True,\n",
        "                             temperature=temperature, top_p=top_p)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompt = \"Nolori safi tem:\"\n",
        "print(\"\\n=== Sample Generation ===\\n\")\n",
        "print(generate(test_prompt))"
      ],
      "id": "ME3UwYPKCtiU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJLOrJfhCtiU"
      },
      "source": [
        "## 6) (Optional) Export to Ollama (manual Modelfile)\n",
        "Ollama can load HF adapters via a **Modelfile**. The simplest path is to export the PEFT adapters and point Ollama to the base model + LoRA.\n",
        "\n",
        "Below we just create a skeleton `Modelfile`. You may need to adjust paths on your machine.\n"
      ],
      "id": "uJLOrJfhCtiU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QuuiXsSaCtiU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "modelfile_path = Path(OUTPUT_DIR)/\"Modelfile\"\n",
        "modelfile_text = f\"\"\"\n",
        "# Example Modelfile for Ollama (adjust paths on your system)\n",
        "FROM {BASE_MODEL}\n",
        "PARAMETER lora {OUTPUT_DIR}\n",
        "TEMPLATE \"You are a helpful assistant.\"\n",
        "\"\"\"\n",
        "modelfile_path.write_text(modelfile_text)\n",
        "print(\"Wrote Modelfile to:\", modelfile_path)\n",
        "print(\"Next on your machine:  ollama create my-continued-model -f\", modelfile_path)\n"
      ],
      "id": "QuuiXsSaCtiU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gWwOyIfCtiV"
      },
      "source": [
        "---\n",
        "### Notes & Tips\n",
        "- For **larger corpora**, increase `EPOCHS` and adjust batch/grad-accum. If you hit OOM, reduce `BATCH_PER_DEVICE` or `MAX_SEQ_LEN`.\n",
        "- To use a different base (e.g., `unsloth/gemma-3-1b-it-bnb-4bit`), just change `BASE_MODEL` and re-run.\n",
        "- If your tokenizer needs special handling (e.g., custom BOS/EOS), set `tokenizer.pad_token = tokenizer.eos_token` (already handled above).\n",
        "- Keep text **clean & UTF-8**. For multilingual corpora, ensure the base model supports that script/tokenization reasonably well.\n",
        "- For full finetuning (no LoRA), load in higher precision and skip `get_peft_model` (requires more VRAM).\n",
        "\n",
        "Happy continued pretraining! ğŸ¦¥"
      ],
      "id": "8gWwOyIfCtiV"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}